{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "u2HGJpiLLc0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to ensure reproducibility, as we will sample data points, we fix the random seed\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "18o1mjo_aWvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Contours and gradients: a quick glance**"
      ],
      "metadata": {
        "id": "uMlc4t7ELROC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s4pUZandUHj"
      },
      "outputs": [],
      "source": [
        "# Define the function f(x1, x2)\n",
        "def f(x1, x2):\n",
        "    return x1**2 + x2**2\n",
        "\n",
        "# Define the gradient of f(x1, x2)\n",
        "def grad_f(x1, x2):\n",
        "    return np.array([2 * x1, 2 * x2])\n",
        "\n",
        "# Define the range of x1 and x2\n",
        "x1_range = np.linspace(-5, 5, 100)\n",
        "x2_range = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Create a meshgrid of x1 and x2 values\n",
        "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
        "\n",
        "# Compute the function values at each point in the meshgrid\n",
        "Z = f(X1, X2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the user to input the coordinates of the point\n",
        "x1 = float(input(\"Enter the x1 coordinate of the point: \"))\n",
        "x2 = float(input(\"Enter the x2 coordinate of the point: \"))\n",
        "\n",
        "# Compute the gradient of f at the user-defined point\n",
        "gradient = grad_f(x1, x2)\n",
        "\n",
        "# Plot contours of the function f(x1, x2)\n",
        "plt.contour(X1, X2, Z, levels=25)\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "\n",
        "# Plot the gradient vector at the user-defined point\n",
        "plt.quiver(x1, x2, gradient[0], gradient[1], color='red', angles='xy', scale_units='xy', scale=1)\n",
        "\n",
        "# Show the plot\n",
        "plt.title('Contours of f(x1, x2) and Gradient at the Point')\n",
        "plt.grid(True)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KN_weV5kdx-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(Mini-batch) Stochastic Gradient Descent**\n",
        "Let us implement a (mini-batch) stochastic gradient descent algorithm for the linear regression model\n",
        "\n",
        "\\begin{align}\n",
        "f_{\\boldsymbol{\\theta}}(x) = \\theta_0+\\theta_1 x.\n",
        "\\end{align}\n",
        "\n",
        "We will simulate a few data points, compute the empirical risk and minimize it using mini-batch SGD. We will investigate how the choice of the initial point of the algorithm, the learning rate and the mini-batch size affect its results."
      ],
      "metadata": {
        "id": "3-fdw3BbZD_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 10000 data points x from Unif(-5, 5)\n",
        "x = np.random.uniform(-5, 5, 100)\n",
        "\n",
        "# Compute the array y = x + epsilon, where epsilon has distribution N(0, 1)\n",
        "epsilon = np.random.normal(0, 1, 100)\n",
        "y = x + epsilon\n",
        "\n",
        "# Linear model function\n",
        "def linear_model(x, theta0, theta1):\n",
        "    return theta0 + theta1 * x"
      ],
      "metadata": {
        "id": "z0w0QKZpMbYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick plot of the generated data points with a chosen \"trend line\"\n",
        "plt.scatter(x,y)\n",
        "\n",
        "x_line = np.arange(-5, 6)\n",
        "theta0, theta1 = 0, 1   # choose your intercept and slope\n",
        "y_line = theta0+ theta1*x_line\n",
        "plt.plot(x_line, y_line,'b-',lw=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s5paqycpbnZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the mini-batch SGD algorithm"
      ],
      "metadata": {
        "id": "BKLArACLbVBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction function for a linear regression\n",
        "def predict(x, theta0, theta1):\n",
        "    return theta0 + theta1 * x\n",
        "\n",
        "# gradients function (as usual, we choose the squared loss)\n",
        "def compute_gradients(x_batch, y_batch, theta0, theta1):\n",
        "    predictions = predict(x_batch, theta0, theta1)\n",
        "\n",
        "    # we computed this gradient already - can you reconstruct these formulae?\n",
        "    error = predictions - y_batch\n",
        "    grad_theta0 = np.mean(error)\n",
        "    grad_theta1 = np.mean(error * x_batch)\n",
        "\n",
        "    return grad_theta0, grad_theta1"
      ],
      "metadata": {
        "id": "Y8pNyOSHPJrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mini-batch stochastic gradient descent (SGD) function\n",
        "def mini_batch_sgd(start_theta0, start_theta1, x, y, learning_rate, num_iterations, batch_size):\n",
        "\n",
        "    # initialization of theta0, theta1\n",
        "    theta0 = start_theta0\n",
        "    theta1 = start_theta1\n",
        "\n",
        "    # in these lists we will collect the theta's for all SGD iterations/steps\n",
        "    theta0_values, theta1_values = [theta0], [theta1]\n",
        "\n",
        "    # we start with the SGD\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "          # mini-batch selection\n",
        "          batch_indices = np.random.choice(range(x.shape[0]), batch_size, replace=False)\n",
        "          x_batch = x[batch_indices]\n",
        "          y_batch = y[batch_indices]\n",
        "\n",
        "          # gradient computation on the mini-batch\n",
        "          grad_theta0, grad_theta1 = compute_gradients(x_batch, y_batch, theta0, theta1)\n",
        "\n",
        "          # parameters update according to the SGD formula\n",
        "          theta0 -= learning_rate * grad_theta0\n",
        "          theta1 -= learning_rate * grad_theta1\n",
        "\n",
        "          # we store the updated values, adding a new element to the lists we initialized above\n",
        "          theta0_values.append(theta0)\n",
        "          theta1_values.append(theta1)\n",
        "\n",
        "    return theta0_values, theta1_values"
      ],
      "metadata": {
        "id": "F7RtOkSmPJ5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let us test our SGD function\n",
        "\n",
        "# initializing theta's (you can also specify your own values)\n",
        "start_theta0 = np.random.rand()\n",
        "start_theta1 = np.random.rand()\n",
        "\n",
        "# choosing the other parameters\n",
        "learning_rate = 0.01\n",
        "num_iterations = 100\n",
        "batch_size = 90       # maximum size = number sampled data points\n",
        "\n",
        "# SGD: collecting all results\n",
        "theta0_seq, theta1_seq = mini_batch_sgd(start_theta0, start_theta1, x, y, learning_rate, num_iterations, batch_size)\n",
        "\n",
        "# SGD: summary of results\n",
        "print('SGD: initialized theta0:', theta0_seq[0])\n",
        "print('SGD: initialized theta1:', theta1_seq[0])\n",
        "print('-------')\n",
        "print('SGD: estimate of theta0:', theta0_seq[-1])\n",
        "print('SGD: estimate of theta1:', theta1_seq[-1])"
      ],
      "metadata": {
        "id": "2jSSqkNgOtWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the sequence of points computed by mini-batch SGD as a time series wrt SGD update step\n",
        "plt.plot(theta1_seq, label='Mini-batch SGD output sequence')\n",
        "\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('theta')\n",
        "plt.title('SGD - Results')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4LFmugkib2Oe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}