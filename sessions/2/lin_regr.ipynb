{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LINEAR REGRESSION**\n",
        "\n",
        "In this exercise, we will run our first example of machine learning model: **a linear regression on a single input variable**. Our exercise comprises two parts.\n",
        "\n",
        "In **PART 1**, we\n",
        "1.   import the required Python libraries,\n",
        "2.   generate some data and plot them,\n",
        "3.   show different linear regression models,\n",
        "4.   compute the loss function on the available data,\n",
        "5.   plot the levels of the loss function.\n",
        "\n",
        "In **PART 2**, we\n",
        "1. \"learn/train\" the linear regression model using OLS,\n",
        "2. \"learn/train\" the linear regression model using gradient descent.\n",
        "\n",
        "**Note** - If you want to modify this notebook, please consider the following Google Colab cheatsheet:\n",
        "https://colab.research.google.com/github/Tanu-N-Prabhu/Python/blob/master/Cheat_sheet_for_Google_Colab.ipynb#scrollTo=8fFhNQ2EGOcR\n",
        "\n",
        "We can start.\n",
        "\n"
      ],
      "metadata": {
        "id": "nbD3Kk_txyek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Python libraries**"
      ],
      "metadata": {
        "id": "R1Brtv930eVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Math library\n",
        "import numpy as np\n",
        "\n",
        "# Plotting library\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HfKPyKtxwHIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PART 1. Introduction to Linear Regression**"
      ],
      "metadata": {
        "id": "n-j6KBgxFoK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **PART 1** we start with some basic exercises with data, linear regressions and loss functions."
      ],
      "metadata": {
        "id": "IhfLn1JS4x9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating some data**"
      ],
      "metadata": {
        "id": "lj5h_Qp40inW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some input / output data\n",
        "x = np.array([0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90])\n",
        "y = np.array([0.67, 0.85, 1.05, 1.00, 1.40, 1.50, 1.30, 1.54, 1.55, 1.68, 1.73, 1.60])\n",
        "\n",
        "# Let us print them out\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "FW-fn-q5wMzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Linear regression**"
      ],
      "metadata": {
        "id": "JoeiSzhJ0mW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 1D linear regression model function\n",
        "def f(x, beta0, beta1):\n",
        "\n",
        "  # Linear regression model - computations\n",
        "  y = beta0 + beta1 * x\n",
        "\n",
        "  return y"
      ],
      "metadata": {
        "id": "EIrVdts-wQlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to help plot the data\n",
        "def plot(x, y, beta0, beta1):\n",
        "\n",
        "    fig,ax = plt.subplots()\n",
        "    ax.scatter(x,y)\n",
        "    plt.xlim([0,2.0])\n",
        "    plt.ylim([0,2.0])\n",
        "    ax.set_xlabel('Input x')\n",
        "    ax.set_ylabel('Output y')\n",
        "\n",
        "    # Draw line on the plot\n",
        "    x_line = np.arange(0,2,0.01)\n",
        "    y_line = f(x_line, beta0, beta1)\n",
        "    plt.plot(x_line, y_line,'b-',lw=2)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Rzy4kJgxwQiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us play around with the linear regression\n",
        "\n",
        "# Set the intercept and slope - examples from the slides: (beta0, beta1) = (0.0, 1.0), (1.0, -0.4), (1.2, -0.1)\n",
        "beta0 = 0.83\n",
        "beta1 = 0.52\n",
        "\n",
        "# Plot the data and the model\n",
        "plot(x, y, beta0, beta1)"
      ],
      "metadata": {
        "id": "bhQ6QCHEwQQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Loss function**"
      ],
      "metadata": {
        "id": "iw46TaEL0qSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the loss\n",
        "def compute_loss(x, y, beta0, beta1):\n",
        "\n",
        "  # We compute the loss in two steps.\n",
        "  # Step 1: I start computing the squared differences, per each data point\n",
        "  loss = (f(x, beta0, beta1)-y)**2\n",
        "\n",
        "  # Step 2: I sum the squared differences\n",
        "  loss = np.sum(loss)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "RkzzWCLNxfd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the loss for our current model\n",
        "loss = compute_loss(x,y, beta0, beta1)\n",
        "print(f'Your Loss = {loss:3.2f}')"
      ],
      "metadata": {
        "id": "tRRfIOd3xfbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us visualize the levels of the loss function using a heatmap\n",
        "\n",
        "# Make a 2D grid of possible phi0 and phi1 values\n",
        "beta0_mesh, beta1_mesh = np.meshgrid(np.arange(0.0,2.0,0.02), np.arange(-1.0,1.0,0.02))\n",
        "\n",
        "# Make a 2D array for the losses\n",
        "all_losses = np.zeros_like(beta1_mesh)\n",
        "\n",
        "# Run through each 2D combination of beta0, beta1 and compute loss\n",
        "for indices,temp in np.ndenumerate(beta1_mesh):\n",
        "    all_losses[indices] = compute_loss(x, y, beta0_mesh[indices], beta1_mesh[indices])"
      ],
      "metadata": {
        "id": "1G8ok9sSxfV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss function as a heatmap\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "fig.set_size_inches(7,7)\n",
        "levels = 256\n",
        "ax.contourf(beta0_mesh, beta1_mesh, all_losses ,levels)\n",
        "levels = 40\n",
        "ax.contour(beta0_mesh, beta1_mesh, all_losses ,levels, colors=['#80808080'])\n",
        "ax.set_ylim([1,-1])\n",
        "ax.set_xlabel('Intercept')\n",
        "ax.set_ylabel('Slope')\n",
        "\n",
        "# Plot the position of the beta's of the linear regression model on the loss function\n",
        "# The best model should have beta's close to where the loss reaches its minimum\n",
        "ax.plot(beta0, beta1, 'ro')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5w_GiFoexfMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PART 2. Learning/Training a Linear Regression (March 15th)**\n",
        "\n",
        "We will consider this second part of the Colab on March 15th, after having discussed gradient descent."
      ],
      "metadata": {
        "id": "W1YR10DB97LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Method 1: OLS using the sklearn function LinearRegression**\n",
        "With this method, all we need is to call the `sklearn` `LinearRegression()` function and specify data in the appropriate shape.\n",
        "\n",
        "The function uses OLS to compute the estimation of the intercept `beta0` and slope `beta1`."
      ],
      "metadata": {
        "id": "dV7oPLRlBsrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us import the linear regression function from sklearn\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "dPVxKhGI-DEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning/training the linear regression on our data (no train vs. test split this time for simplicity)\n",
        "lin_reg = LinearRegression().fit(x.reshape(-1, 1), y.reshape(-1, 1)) # we need to reshape inputs\n",
        "\n",
        "print('Estimated intercept:', lin_reg.intercept_.round(2)) # let us keep only two decimal places (you can do it in different ways)\n",
        "print('Estimated slope:', lin_reg.coef_.round(2)) # let us keep only two decimal places (you can do it in different ways)"
      ],
      "metadata": {
        "id": "B1GFenAD-P38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the learned/trained model 'lin_reg' computes predictions on new data points (we need to shape them correctly, however - arrays of shape (1, 1))\n",
        "lin_reg.predict(np.array([[0.15]]))"
      ],
      "metadata": {
        "id": "PynHG7u-_10G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Method 2: Gradient Descent (by hand)**\n",
        "With this method, we will define a gradient descent function by hand and use it to compute the estimates of the intercept `beta0` and the slope `beta1`.\n",
        "\n",
        "Key here is the analytical computations of the gradient of the loss function $L(\\boldsymbol{\\theta}|x_i,y_i)$ at each point $(x_i,y_i)$ - that is each element of the arrays `x` and `y`.\n",
        "We have:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial L(\\boldsymbol{\\theta}|x_i,y_i)}{\\partial\\theta_0} &=& -2(y_i-\\theta_0-\\theta_1 x_i), \\\\\n",
        "\\frac{\\partial L(\\boldsymbol{\\theta}|x_i,y_i)}{\\partial\\theta_1} &=& x_i \\frac{\\partial L(\\boldsymbol{\\theta}|x_i,y_i)}{\\partial\\theta_0}.\n",
        "\\end{eqnarray}\n",
        "\n",
        "We can start computing the gradients and implement the gradient descent algorithm by hand."
      ],
      "metadata": {
        "id": "v5akstq_IOcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent function\n",
        "def gradient_descent_func(x, y, beta0_start, beta1_start, eta, t_max):\n",
        "\n",
        "    # initialization: we are at step 0\n",
        "    t=0\n",
        "\n",
        "    # initialization: specify the starting values for the beta's\n",
        "    beta0 = beta0_start\n",
        "    beta1 = beta1_start\n",
        "\n",
        "    # we implement the gradient descent algorithm update rule\n",
        "    while t < t_max:\n",
        "\n",
        "      # gradient of the loss wrt beta's for each element of the arrays x, y\n",
        "      pder_loss_beta0 = -2 * (y-beta0-beta1 * x)\n",
        "      pder_loss_beta1 = x * pder_loss_beta0\n",
        "\n",
        "      # gradient descent: updating beta's\n",
        "      beta0 = beta0 - eta * np.mean(pder_loss_beta0)\n",
        "      beta1 = beta1 - eta * np.mean(pder_loss_beta1)\n",
        "\n",
        "      # increment step by one\n",
        "      t=t+1\n",
        "\n",
        "    # when t_max is reached exit giving the beta's at t_max\n",
        "    return beta0, beta1"
      ],
      "metadata": {
        "id": "IeI3kj77KPlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us estimate beta's using our gradient descent function\n",
        "# Good choices: (0.10, 0.10, 0.10, 100), (1, 1, 0.10, 100)\n",
        "# Hint: Try (0.10, 0.10, 1, x)...see what happens incrementing x one by one...\n",
        "\n",
        "beta0_start=1\n",
        "beta1_start=1\n",
        "eta=0.1\n",
        "t_max=100\n",
        "\n",
        "gradient_descent_func(x, y, beta0_start, beta1_start, eta, t_max)"
      ],
      "metadata": {
        "id": "0IgFUPNINNTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}