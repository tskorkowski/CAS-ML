{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQpivMlifkIGyINMMUn/Gz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Abstract Neurons: An Introduction to Deep Learning\n","\n","In this Colab, we will introduce a few examples of abstract neurons. We start by introducing a few activation functions, then moving to the perceptron and sigmoid neurons. We will discuss the classification boundary of sigmoid neurons before showing that feedforward neural networks with two one hidden layer can learn XOR."],"metadata":{"id":"VEPdtX5HnB8e"}},{"cell_type":"markdown","source":["## Overview of some activation functions\n","We start with plotting a few activation functions that are commonly used in deep learning applications."],"metadata":{"id":"Az4bMSxVqRpw"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the activation functions\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def tanh(x):\n","    return np.tanh(x)\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def param_relu(x, alpha=0.1):\n","    return np.where(x > 0, x, x * alpha)\n","\n","# Generate a range of values\n","x = np.linspace(-10, 10, 100)\n","\n","# Plot each activation function\n","plt.figure(figsize=(12, 8))\n","\n","plt.subplot(2, 2, 1)\n","plt.plot(x, sigmoid(x), label='Sigmoid', linewidth=0.75)\n","plt.title('Sigmoid Activation Function')\n","plt.grid(True)\n","\n","plt.subplot(2, 2, 2)\n","plt.plot(x, tanh(x), label='Tanh', linewidth=0.75)\n","plt.title('Tanh Activation Function')\n","plt.grid(True)\n","\n","plt.subplot(2, 2, 3)\n","plt.plot(x, relu(x), label='ReLU', linewidth=0.75)\n","plt.title('ReLU Activation Function')\n","plt.grid(True)\n","\n","plt.subplot(2, 2, 4)\n","plt.plot(x, param_relu(x), label='PReLU', linewidth=0.75)\n","plt.title('Parametric ReLU Activation Function')\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"zUV8l8aHnKAn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The perceptron: learning the AND function\n","We implement a perceptron that learns the AND function. You may want to change the weights of the perceptron."],"metadata":{"id":"KH2BIatkqXTI"}},{"cell_type":"code","source":["# Define the perceptron function\n","def perceptron_and(x1, x2):\n","    weight0 = -0.5 #-1.5\n","    weight1 = 0.18 #1.0\n","    weight2 = 0.35 #1.0\n","    output = weight0 + weight1*x1 + weight2*x2\n","    return 1 if output > 0 else 0\n","\n","# Example inputs and output for AND\n","print(\"AND(0, 0) = 0 versus\", perceptron_and(0, 0))\n","print(\"AND(0, 1) = 0 versus\", perceptron_and(0, 1))\n","print(\"AND(1, 0) = 0 versus\", perceptron_and(1, 0))\n","print(\"AND(1, 1) = 1 versus\", perceptron_and(1, 1))"],"metadata":{"id":"HFifSdpontvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let us plot the decision boundary of the perceptron implementing AND\n","# We already know that the boundary is a line in the (x1, x2) plane - remember the slides\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Points to plot\n","points = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n","\n","# AND logic outputs for the points\n","outputs = np.array([0, 0, 0, 1])\n","\n","# Perceptron parameters for AND logic\n","weights = np.array([1, 1])\n","weight0 = -1.5\n","\n","# Plot points\n","for point, output in zip(points, outputs):\n","    plt.plot(point[0], point[1], 'ro' if output == 1 else 'bo')\n","\n","# Calculate and plot decision boundary\n","x1 = np.linspace(-0.5, 1.5, 100)\n","x2 = -(weight0 + weights[0] * x1 ) / weights[1]\n","\n","plt.plot(x1, x2, 'g--', label='Decision Boundary')\n","\n","plt.xlim(-0.5, 1.5)\n","plt.ylim(-0.5, 1.5)\n","plt.xlabel('x1')\n","plt.ylabel('x2')\n","plt.title('Perceptron Decision Boundary and Points')\n","plt.axhline(0, color='black',linewidth=0.5)\n","plt.axvline(0, color='black',linewidth=0.5)\n","plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"vRxXkzWZorB_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The sigmoid neuron\n","We investigate the sigmoid neuron on two inputs."],"metadata":{"id":"NifamKbTqhkS"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the sigmoid activation function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Define a sigmoid neuron for 2D inputs\n","def sigmoid_neuron_2d(inputs, weight0, weights):\n","    z = weight0 + np.dot(inputs, weights)\n","    return sigmoid(z)\n","\n","# Initialize weights and bias randomly\n","weight0 = np.random.rand()  # Random bias (=weight0)\n","weights = np.random.rand(2)  # Random weights for 2D inputs\n","\n","# Generate 100 2D inputs from the uniform distribution\n","inputs = np.random.uniform(-1, 1, (100, 2))\n","\n","# Compute the sigmoid neuron output for the inputs\n","outputs = sigmoid_neuron_2d(inputs, weight0, weights)"],"metadata":{"id":"c3zTRzjTqkxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(outputs, bins=30, color='skyblue', edgecolor='black')\n","plt.title('Histogram of Outputs of the Sigmoid Neuron')\n","plt.xlabel('Output Value')\n","plt.ylabel('Frequency')\n","plt.show()"],"metadata":{"id":"U-sjs9Rgqkuy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Classification rule of the outputs of the sigmoid neuron - we can use a threshold = 0.5\n","classification = [1 if output >= 0.5 else 0 for output in outputs]\n","\n","# Plotting classified inputs\n","plt.scatter(inputs[:, 0], inputs[:, 1], c=classification, cmap='viridis', marker='o', edgecolor='none')\n","plt.ylim(-1, 1)\n","plt.title('2D Inputs Classified by the Sigmoid Neuron')\n","plt.xlabel('x_1')\n","plt.ylabel('x_2')\n","plt.colorbar(ticks=[0, 1], label='Class')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"t3c3NxSDqksE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Adding the decision boundary to the classification of the outputs of the sigmoid neuron\n","Let us derive the equation of the decision boundary of the sigmoid neuron by two-dimensional inputs. We start again with the sigmoid function $\\sigma(z)$:\n","\n","\\begin{align}\n","\\sigma(z) = \\frac{1}{1 + e^{-z}}\n","\\end{align}\n","\n","By definition, the decision boundary of the sigmoid neuron is the set of points $(x_1,x_2)$ that satify\n","\n","\\begin{align}\n","\\sigma(z) = c,~~ c\\in (0,1) ~~(*)\n","\\end{align}\n","\n","where $z=w_0 + w_1 x_1 + w_2 x_2$. We solve $(*)$ for $z$:\n","\n","\\begin{align}\n","c = \\frac{1}{1 + e^{-z}} ⇔ e^{-z} = \\frac{1}{c} - 1 ⇔ -z = \\log\\left(\\frac{1}{c} - 1\\right) ⇔ z = -\\log\\left(\\frac{1}{c} - 1\\right).\n","\\end{align}\n","\n","Using the definition of $z$ yields:\n","\n","\\begin{align}\n","w_1x_1 + w_2x_2 + w_0 = -\\log\\left(\\frac{1}{c} - 1\\right),~ c\\in (0,1).\n","\\end{align}\n","\n","This is the equation for the decision boundary of the sigmoid neuron.\n","For instance, if $w_2\\neq 0$, rearranging for $x_2$ allows writing the decision boundary as follows:\n","\n","\\begin{align}\n","x_2 = -\\frac{w_1}{w_2}x_1 - \\frac{w_0}{w_2} - \\frac{1}{w_2}\\log\\left(\\frac{1}{c} - 1\\right).\n","\\end{align}"],"metadata":{"id":"sZByHUh0x33v"}},{"cell_type":"code","source":["# Classification rule and the classification boundary\n","thr = 0.5 # choose the threshold\n","classification_thr = [1 if output >= thr else 0 for output in outputs]\n","\n","# decision boundary\n","w_1, w_2 = weights\n","w_0 = weight0\n","y_boundary_thr = (-w_1 / w_2) * inputs[:, 0] - (w_0 / w_2) - (1 / w_2) * np.log(1/thr - 1) #w_2 is not equal to zero in our example\n","\n","# Plotting\n","plt.scatter(inputs[:, 0], inputs[:, 1], c=classification_thr, cmap='viridis', marker='o', edgecolor='none')\n","plt.plot(inputs[:, 0], y_boundary_thr, color='red', label='Decision Boundary', linewidth=0.5)\n","plt.ylim(-1, 1)\n","plt.title('2D Inputs Classified by Sigmoid Neuron')\n","plt.xlabel('x_1')\n","plt.ylabel('x_2')\n","plt.colorbar(ticks=[0, 1], label='Class')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"zUjeihB1qkmv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implementing XOR with a feedforward neural network (FNN)\n","Finally, we check in Python that a feedforward neural network with one hidden layer can implement the function XOR. Remember that a perceptron cannot implement it. This is an important result in the history of deep learning."],"metadata":{"id":"mZDGRfUSnvPC"}},{"cell_type":"code","source":["# Heaviside activation function\n","def heaviside(x):\n","    return np.where(x >= 0, 1, 0)\n","\n","# Generate a range of values\n","x = np.linspace(-10, 10, 100)\n","\n","plt.plot(x, heaviside(x), label='Heaviside function', linewidth=0.75)\n","plt.title('Heaviside Activation function')\n","plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n","plt.show()"],"metadata":{"id":"fjVUvo_OuZ_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# XOR function using a FNN with 2 inputs, 2 hidden neurons, and 1 output neuron\n","\n","def xor_neural_network(x):\n","    # Weights and biases for the hidden layer\n","\n","    W1 = np.array([[1, 1], [1, 1]]) # Weights connecting inputs to the hidden layer\n","    w1 = np.array([-0.5, -1.5])     # Biases for the hidden layer\n","\n","    # Weights and biases for the output layer\n","    W2 = np.array([1, -2]) # Weights connecting the hidden layer to the output layer\n","    w2 = -0.5              # Bias for the output layer\n","\n","    # Composing the output of the FNN\n","    # from inputs to hidden layer\n","    q_hidden = np.dot(x, W1) + w1\n","    h_hidden = heaviside(q_hidden)\n","\n","    # from hidden layer to output later\n","    z_output = np.dot(h_hidden, W2) + w2\n","    y_output = heaviside(z_output)\n","\n","    return y_output"],"metadata":{"id":"Nl_Bw3U5nzhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let us test the FNN: does it really implement XOR?\n","inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","for x in inputs:\n","    print(f\"Input: {x} Output FNN: {xor_neural_network(x)}\")"],"metadata":{"id":"ctRcNuCPyDTN"},"execution_count":null,"outputs":[]}]}