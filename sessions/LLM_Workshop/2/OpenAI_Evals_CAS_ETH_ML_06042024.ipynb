{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m538Ie5Ruhld"
      },
      "source": [
        "OpenAI eval notes prepared for [CAS ETH in ML in Finance and Insurance](https://finsuretech.ethz.ch/continuing-education/cas-ml-in-finance-and-insurance.html) by Eleni Verteouri."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zcphh1zM_j3Z"
      },
      "source": [
        "# Getting Started with OpenAI Evals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "6vF_ep5D_j3b"
      },
      "source": [
        "The [OpenAI Evals](https://github.com/openai/evals/tree/main) framework consists of\n",
        "1. A framework to evaluate an LLM (large language model) or a system built on top of an LLM.\n",
        "2. An open-source registry of challenging evals\n",
        "\n",
        "This notebook will cover:\n",
        "* Introduction to Evaluation and the [OpenAI Evals](https://github.com/openai/evals/tree/main) library\n",
        "* Building an Eval\n",
        "* Running an Eval\n",
        "\n",
        "#### What are evaluations/ `evals`?\n",
        "\n",
        "Evaluation is the process of validating and testing the outputs that your LLM applications are producing. Having strong evaluations (\"evals\") will mean a more stable, reliable application that is resilient to code and model changes. An eval is a task used to measure the quality of the output of an LLM or LLM system. Given an input prompt, an output is generated. We evaluate this output with a set of ideal answers and find the quality of the LLM system.\n",
        "\n",
        "#### Importance of Evaluations\n",
        "\n",
        "If you are building with foundational models like `GPT-4`, creating high quality evals is one of the most impactful things you can do. Developing AI solutions involves an iterative design process. [Without evals, it can be very difficult and time intensive to understand](https://youtu.be/XGJNo8TpuVA?feature=shared&t=1089) how different model versions and prompts might affect your use case.\n",
        "\n",
        "With OpenAI’s [continuous model upgrades](https://platform.openai.com/docs/models/continuous-model-upgrades), evals allow you to efficiently test model performance for your use cases in a standardized way. Developing a suite of evals customized to your objectives will help you quickly and effectively understand how new models may perform for your use cases. You can also make evals a part of your CI/CD pipeline to make sure you achieve the desired accuracy before deploying.\n",
        "\n",
        "#### Types of evals\n",
        "\n",
        "There are two main ways we can evaluate/grade completions: writing some validation logic in code\n",
        "or using the model itself to inspect the answer. We’ll introduce each with some examples.\n",
        "\n",
        "**Writing logic for answer checking**\n",
        "\n",
        "The simplest and most common type of eval has an input and an ideal response or answer. For example,\n",
        "we can have an eval sample where the input is \"What year was Obama elected president for the first\n",
        "time?\" and the ideal answer is \"2008\". We feed the input to a model and get the completion. If the model\n",
        "says \"2008\", it is then graded as correct. We can write a string match to check if the completion includes the phrase \"2008\". If it does, we consider it correct.\n",
        "\n",
        "Consider another eval where the input is to generate valid JSON: We can write some code that\n",
        "attempts to parse the completion as JSON and then considers the completion correct if it is\n",
        "parsable.\n",
        "\n",
        "**Model grading: A two stage process where the model first answers the question, then we ask a\n",
        "model to look at the response to check if it’s correct.**\n",
        "\n",
        "Consider an input that asks the model to write a funny joke. The model then generates a\n",
        "completion. We then create a new input to the model to answer the question: \"Is this following\n",
        "joke funny? First reason step by step, then answer yes or no\" that includes the completion.\" We\n",
        "finally consider the original completion correct if the new model completion ends with \"yes\".\n",
        "\n",
        "Model grading works best with the latest, most powerful models like `GPT-4` and if we give them the ability\n",
        "to reason before making a judgment. Model grading will have an error rate, so it is important to validate\n",
        "the performance with human evaluation before running the evals at scale. For best results, it makes\n",
        "sense to use a different model to do grading from the one that did the completion, like using `GPT-4` to\n",
        "grade `GPT-3.5` answers.\n",
        "\n",
        "#### OpenAI Eval Templates\n",
        "\n",
        "In using evals, we have discovered several \"templates\" that accommodate many different benchmarks. We have implemented these templates in the OpenAI Evals library to simplify the development of new evals. For example, we have defined 2 types of eval templates that can be used out of the box:\n",
        "\n",
        "* **Basic Eval Templates**: These contain deterministic functions to compare the output to the ideal_answers. In cases where the desired model response has very little variation, such as answering multiple choice questions or simple questions with a straightforward answer, we have found this following templates to be useful.\n",
        "\n",
        "* **Model-Graded Templates**: These contain functions where an LLM compares the output to the ideal_answers and attempts to judge the accuracy. In cases where the desired model response can contain significant variation, such as answering an open-ended question, we have found that using the model to grade itself is a viable strategy for automated evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9efyb5_j3c"
      },
      "source": [
        "### Getting Setup\n",
        "\n",
        "First, go to [github.com/openai/evals](https://github.com/openai/evals), clone the repository with `git clone git@github.com:openai/evals.git` and go through the [setup instructions](https://github.com/openai/evals).\n",
        "\n",
        "To run evals later in this notebook, you will need to set up and specify your OpenAI API key. After you obtain an API key, specify it using the `OPENAI_API_KEY` environment variable.\n",
        "\n",
        "Please be aware of the costs associated with using the API when running evals."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the OPENAI_API_KEY environment variable\n",
        "import os\n",
        "import openai\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your key\"\n",
        "\n",
        "# Access the environment variable\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "wtcjBh967VM8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6UXzhPkA_j3d"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "q7d_m6QE_j3f"
      },
      "source": [
        "## Building an evaluation for OpenAI Evals framework\n",
        "\n",
        "At its core, an eval is a dataset and an eval class that is defined in a YAML file. To start creating an eval, we need\n",
        "\n",
        "1. The test dataset in the `jsonl` format.\n",
        "2. The eval template to be used\n",
        "\n",
        "### Creating the eval dataset\n",
        "Lets create a dataset for a use case where we are evaluating the model's ability to generate syntactically correct SQL. In this use case, we have a series of tables that are related to car manufacturing\n",
        "\n",
        "First we will need to create a system prompt that we would like to evaluate. We will pass in instructions for the model as well as an overview of the table structure:\n",
        "```\n",
        "\"TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]\"\n",
        "```\n",
        "\n",
        "For this prompt, we can ask a specific question:\n",
        "```\n",
        "\"Q: how many car makers are their in germany?\"\n",
        "```\n",
        "\n",
        "And we have an expected answer:\n",
        "```\n",
        "\"A: SELECT count ( * )  FROM CAR_MAKERS AS T1 JOIN COUNTRIES AS T2 ON T1.Country   =   T2.CountryId WHERE T2.CountryName   =   'germany'\"\n",
        "```\n",
        "\n",
        "The dataset needs to be in the following format:\n",
        "```\n",
        "\"input\": [{\"role\": \"system\", \"content\": \"<input prompt>\"}, {\"role\": \"user\", \"content\": <user input>}, \"ideal\": \"correct answer\"]\n",
        "```\n",
        "\n",
        "Putting it all together, we get:\n",
        "```\n",
        "{\"input\": [{\"role\": \"system\", \"content\": \"TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]\\n\"}, {\"role\": \"system\", \"content\": \"Q: how many car makers are their in germany\"}, \"ideal\": [\"A: SELECT count ( * )  FROM CAR_MAKERS AS T1 JOIN COUNTRIES AS T2 ON T1.Country   =   T2.CountryId WHERE T2.CountryName   =   'germany'\"]}\n",
        "```\n",
        "\n",
        "\n",
        "One way to speed up the process of building eval datasets, is to use `GPT-4` to generate synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "emWRk3dg_j3f",
        "outputId": "85b097f5-25be-45fc-bb88-3deb3b98bd9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the average horsepower for cars made in countries within the continent of Asia?\n",
            "A: SELECT AVG(cars_data.Horsepower) FROM cars_data JOIN car_names ON cars_data.Id = car_names.MakeId JOIN car_makers ON car_names.MakeId = car_makers.Id JOIN countries ON car_makers.Country = countries.CountryId JOIN continents ON countries.Continent = continents.ContId WHERE continents.Continent = 'Asia'\n",
            "\n",
            "Q: What is the average horsepower for cars made in the USA?\n",
            "A: SELECT AVG(cars_data.Horsepower) FROM cars_data JOIN car_names ON cars_data.Id = car_names.MakeId JOIN model_list ON car_names.Model = model_list.Model JOIN car_makers ON model_list.Maker = car_makers.Id JOIN countries ON car_makers.Country = countries.CountryId WHERE countries.CountryName = 'USA'\n",
            "\n",
            "Q: What is the average horsepower of cars made by a maker from the continent of Europe?\n",
            "A: SELECT AVG(cars_data.Horsepower) FROM cars_data JOIN car_names ON cars_data.Id = car_names.MakeId JOIN model_list ON car_names.Model = model_list.Model JOIN car_makers ON model_list.Maker = car_makers.Id JOIN countries ON car_makers.Country = countries.CountryId JOIN continents ON countries.Continent = continents.ContId WHERE continents.Continent = 'Europe'\n",
            "\n",
            "Q: What is the average weight of cars produced by makers in Europe?\n",
            "A: SELECT AVG(cars_data.Weight) FROM cars_data JOIN car_names ON cars_data.Id = car_names.MakeId JOIN model_list ON car_names.Model = model_list.Model JOIN car_makers ON model_list.Maker = car_makers.Id JOIN countries ON car_makers.Country = countries.CountryId JOIN continents ON countries.Continent = continents.ContId WHERE continents.Continent = 'Europe'\n",
            "\n",
            "Q: Which car maker has the highest average horsepower across all models?\n",
            "A: SELECT cm.Maker, AVG(cd.Horsepower) as AvgHorsepower FROM car_makers cm JOIN car_names cn ON cm.Id = cn.MakeId JOIN cars_data cd ON cn.MakeId = cd.Id GROUP BY cm.Maker ORDER BY AvgHorsepower DESC LIMIT 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Use GPT-4 to generate synthetic data\n",
        "# Define the system prompt and user input (these should be filled as per the specific use case)\n",
        "system_prompt = \"\"\"You are a helpful assistant that can ask questions about a database table and write SQL queries to answer the question.\n",
        "    A user will pass in a table schema and your job is to return a question answer pairing. The question should relevant to the schema of the table,\n",
        "    and you can speculate on its contents. You will then have to generate a SQL query to answer the question. Below are some examples of what this should look like.\n",
        "\n",
        "    Example 1\n",
        "    ```````````\n",
        "    User input: Table museum, columns = [*,Museum_ID,Name,Num_of_Staff,Open_Year]\\nTable visit, columns = [*,Museum_ID,visitor_ID,Num_of_Ticket,Total_spent]\\nTable visitor, columns = [*,ID,Name,Level_of_membership,Age]\\nForeign_keys = [visit.visitor_ID = visitor.ID,visit.Museum_ID = museum.Museum_ID]\\n\n",
        "    Assistant Response:\n",
        "    Q: How many visitors have visited the museum with the most staff?\n",
        "    A: SELECT count ( * )  FROM VISIT AS T1 JOIN MUSEUM AS T2 ON T1.Museum_ID   =   T2.Museum_ID WHERE T2.Num_of_Staff   =   ( SELECT max ( Num_of_Staff )  FROM MUSEUM )\n",
        "    ```````````\n",
        "\n",
        "    Example 2\n",
        "    ```````````\n",
        "    User input: Table museum, columns = [*,Museum_ID,Name,Num_of_Staff,Open_Year]\\nTable visit, columns = [*,Museum_ID,visitor_ID,Num_of_Ticket,Total_spent]\\nTable visitor, columns = [*,ID,Name,Level_of_membership,Age]\\nForeign_keys = [visit.visitor_ID = visitor.ID,visit.Museum_ID = museum.Museum_ID]\\n\n",
        "    Assistant Response:\n",
        "    Q: What are the names who have a membership level higher than 4?\n",
        "    A: SELECT Name   FROM VISITOR AS T1 WHERE T1.Level_of_membership   >   4\n",
        "    ```````````\n",
        "\n",
        "    Example 3\n",
        "    ```````````\n",
        "    User input: Table museum, columns = [*,Museum_ID,Name,Num_of_Staff,Open_Year]\\nTable visit, columns = [*,Museum_ID,visitor_ID,Num_of_Ticket,Total_spent]\\nTable visitor, columns = [*,ID,Name,Level_of_membership,Age]\\nForeign_keys = [visit.visitor_ID = visitor.ID,visit.Museum_ID = museum.Museum_ID]\\n\n",
        "    Assistant Response:\n",
        "    Q: How many tickets of customer id 5?\n",
        "    A: SELECT count ( * )  FROM VISIT AS T1 JOIN VISITOR AS T2 ON T1.visitor_ID   =   T2.ID WHERE T2.ID   =   5\n",
        "    ```````````\n",
        "    \"\"\"\n",
        "\n",
        "user_input = \"Table car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]\"\n",
        "\n",
        "messages = [{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_prompt\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_input\n",
        "    }\n",
        "]\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    messages=messages,\n",
        "    temperature=0.7,\n",
        "    n=5\n",
        ")\n",
        "\n",
        "for choice in completion.choices:\n",
        "    print(choice.message.content + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uN2ELTa_j3i"
      },
      "source": [
        "Once we have the synthetic data, we need to convert it to match the format of the eval dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K9rAwoxn_j3i",
        "outputId": "4aa220d6-b807-4bea-e2ce-88efb169f043",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': [{'role': 'system', 'content': 'TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]'}, {'role': 'user', 'content': 'What is the average horsepower for cars made in countries within the continent of Asia?'}], 'ideal': \"SELECT AVG(cars_data.Horsepower) FROM cars_data JOIN car_names ON cars_data.Id = car_names.MakeId JOIN car_makers ON car_names.MakeId = car_makers.Id JOIN countries ON car_makers.Country = countries.CountryId JOIN continents ON countries.Continent = continents.ContId WHERE continents.Continent = 'Asia'\"}\n",
            "{'input': [{'role': 'system', 'content': 'TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]'}, {'role': 'user', 'content': 'What is the average horsepower for cars made in the USA?'}], 'ideal': \"SELECT AVG(cars_data.Horsepower) FROM cars_data JOIN car_names ON cars_data.Id = car_names.MakeId JOIN model_list ON car_names.Model = model_list.Model JOIN car_makers ON model_list.Maker = car_makers.Id JOIN countries ON car_makers.Country = countries.CountryId WHERE countries.CountryName = 'USA'\"}\n",
            "{'input': [{'role': 'system', 'content': 'TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]'}, {'role': 'user', 'content': 'What is the average horsepower of cars made by a maker from the continent of Europe?'}], 'ideal': \"SELECT AVG(cars_data.Horsepower) FROM cars_data JOIN car_names ON cars_data.Id = car_names.MakeId JOIN model_list ON car_names.Model = model_list.Model JOIN car_makers ON model_list.Maker = car_makers.Id JOIN countries ON car_makers.Country = countries.CountryId JOIN continents ON countries.Continent = continents.ContId WHERE continents.Continent = 'Europe'\"}\n",
            "{'input': [{'role': 'system', 'content': 'TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]'}, {'role': 'user', 'content': 'What is the average weight of cars produced by makers in Europe?'}], 'ideal': \"SELECT AVG(cars_data.Weight) FROM cars_data JOIN car_names ON cars_data.Id = car_names.MakeId JOIN model_list ON car_names.Model = model_list.Model JOIN car_makers ON model_list.Maker = car_makers.Id JOIN countries ON car_makers.Country = countries.CountryId JOIN continents ON countries.Continent = continents.ContId WHERE continents.Continent = 'Europe'\"}\n",
            "{'input': [{'role': 'system', 'content': 'TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]'}, {'role': 'user', 'content': 'Which car maker has the highest average horsepower across all models?'}], 'ideal': 'SELECT cm.Maker, AVG(cd.Horsepower) as AvgHorsepower FROM car_makers cm JOIN car_names cn ON cm.Id = cn.MakeId JOIN cars_data cd ON cn.MakeId = cd.Id GROUP BY cm.Maker ORDER BY AvgHorsepower DESC LIMIT 1'}\n"
          ]
        }
      ],
      "source": [
        "eval_data = []\n",
        "input_prompt = \"TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]\"\n",
        "\n",
        "for choice in completion.choices:\n",
        "    question = choice.message.content.split(\"Q: \")[1].split(\"\\n\")[0]  # Extracting the question\n",
        "    answer = choice.message.content.split(\"\\nA: \")[1].split(\"\\n\")[0]  # Extracting the answer\n",
        "    eval_data.append({\n",
        "        \"input\": [\n",
        "            {\"role\": \"system\", \"content\": input_prompt},\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "        \"ideal\": answer\n",
        "    })\n",
        "\n",
        "for item in eval_data:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZQfQDve_j3k"
      },
      "source": [
        "Next we need to create the eval registry to run it in the framework.\n",
        "\n",
        "The evals framework requires a `.yaml` file structured with the following properties:\n",
        "* `id` - An identifier for your eval\n",
        "* `description` - A short description of your eval\n",
        "* `disclaimer` - An additional notes about your eval\n",
        "* `metrics` - There are three types of eval metrics we can choose from: match, includes, fuzzyMatch\n",
        "\n",
        "For our eval, we will configure the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L-SxJctG_j3l",
        "outputId": "14d6a57b-3da3-4fb4-b7bb-9c959fe04a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nspider-sql:\\n  id: spider-sql.dev.v0\\n  metrics: [accuracy]\\n  description: Eval that scores SQL code from 194 examples in the Spider Text-to-SQL test dataset. The problems are selected by taking the first 10 problems for each database that appears in the test set.\\n    Yu, Tao, et al. \"Spider; A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task.\" Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, https://doi.org/10.18653/v1/d18-1425.\\n  disclaimer: Problems are solved zero-shot with no prompting other than the schema; performance may improve with training examples, fine tuning, or a different schema format. Evaluation is currently done through model-grading, where SQL code is not actually executed; the model may judge correct SQL to be incorrect, or vice-versa.\\n\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\"\"\"\n",
        "spider-sql:\n",
        "  id: spider-sql.dev.v0\n",
        "  metrics: [accuracy]\n",
        "  description: Eval that scores SQL code from 194 examples in the Spider Text-to-SQL test dataset. The problems are selected by taking the first 10 problems for each database that appears in the test set.\n",
        "    Yu, Tao, et al. \\\"Spider; A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task.\\\" Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, https://doi.org/10.18653/v1/d18-1425.\n",
        "  disclaimer: Problems are solved zero-shot with no prompting other than the schema; performance may improve with training examples, fine tuning, or a different schema format. Evaluation is currently done through model-grading, where SQL code is not actually executed; the model may judge correct SQL to be incorrect, or vice-versa.\n",
        "\n",
        "  \"\"\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "D7hPml6P_j3m"
      },
      "source": [
        "## Running an evaluation\n",
        "\n",
        "We can run this eval using the `oaieval` CLI. To get setup, install the library: `pip install .` (if you are running the [OpenAI Evals library](github.com/openai/evals) locally) or `pip install oaieval` if you are running an existing eval.\n",
        "\n",
        "Then, run the eval using the CLI: `oaieval gpt-3.5-turbo spider-sql`\n",
        "\n",
        "This command expects a model name and an eval set name. Note that we provide two command line interfaces (CLIs): `oaieval` for running a single eval and `oaievalset` for running a set of evals. The valid eval names are specified in the YAML files under `evals/registry/evals` and their corresponding implementations can be found in `evals/elsuite`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OTI0UxTH_j3m"
      },
      "outputs": [],
      "source": [
        "!pip install evals --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNqhSV1-_j3o"
      },
      "source": [
        "The `oaieval` CLI can accept various flags to modify the default behavior. You can run `oaieval --help` to see a full list of CLI options.\n",
        "\n",
        "After running that command, you’ll see the final report of accuracy printed to the console, as well as a file path to a temporary file that contains the full report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBJYt_-c_j3o"
      },
      "source": [
        "These CLIs can accept various flags to modify their default behavior. You can run `oaieval --help` to see a full list of CLI options.\n",
        "\n",
        "After running that command, you’ll see the final report of accuracy printed to the console, as well as a file path to a temporary file that contains the full report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Wvry8HVB_j3p",
        "outputId": "f173424e-1ac7-44de-ff67-2dd7d51f1894",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-03-29 21:04:40,400] [registry.py:257] Loading registry from /usr/local/lib/python3.10/dist-packages/evals/registry/evals\n",
            "[2024-03-29 21:04:41,850] [registry.py:257] Loading registry from /root/.evals/evals\n",
            "[2024-03-29 21:04:41,858] [oaieval.py:189] \u001b[1;35mRun started: 240329210441MG44YXKV\u001b[0m\n",
            "[2024-03-29 21:04:41,860] [registry.py:257] Loading registry from /usr/local/lib/python3.10/dist-packages/evals/registry/modelgraded\n",
            "[2024-03-29 21:04:41,996] [registry.py:257] Loading registry from /root/.evals/modelgraded\n",
            "[2024-03-29 21:04:41,996] [data.py:90] Fetching /usr/local/lib/python3.10/dist-packages/evals/registry/data/sql/spider_sql.jsonl\n",
            "[2024-03-29 21:04:42,000] [eval.py:36] Evaluating 25 samples\n",
            "[2024-03-29 21:04:42,054] [eval.py:144] Running in threaded mode with 10 threads!\n",
            "  0% 0/25 [00:00<?, ?it/s][2024-03-29 21:04:43,283] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:43,416] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:43,520] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:43,540] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:43,547] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:43,673] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:43,676] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:43,963] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:43,991] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:44,066] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:44,909] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "  4% 1/25 [00:02<01:07,  2.81s/it][2024-03-29 21:04:44,959] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:45,391] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 12% 3/25 [00:03<00:19,  1.10it/s][2024-03-29 21:04:45,547] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 16% 4/25 [00:03<00:13,  1.55it/s][2024-03-29 21:04:45,704] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:45,714] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 20% 5/25 [00:03<00:09,  2.05it/s][2024-03-29 21:04:45,723] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:46,032] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 28% 7/25 [00:03<00:05,  3.05it/s][2024-03-29 21:04:46,172] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:46,258] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:46,370] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 32% 8/25 [00:04<00:05,  3.02it/s][2024-03-29 21:04:46,429] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:46,593] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:46,733] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:46,753] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:47,060] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 36% 9/25 [00:04<00:06,  2.34it/s][2024-03-29 21:04:47,118] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:47,184] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:48,260] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:48,504] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 44% 11/25 [00:06<00:07,  1.80it/s][2024-03-29 21:04:48,625] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 48% 12/25 [00:06<00:05,  2.21it/s][2024-03-29 21:04:48,685] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:48,971] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 56% 14/25 [00:06<00:03,  2.95it/s][2024-03-29 21:04:49,239] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:49,344] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:49,453] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 60% 15/25 [00:07<00:03,  2.69it/s][2024-03-29 21:04:49,678] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:49,727] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 64% 16/25 [00:07<00:03,  2.88it/s][2024-03-29 21:04:49,771] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:49,958] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 68% 17/25 [00:07<00:02,  3.15it/s][2024-03-29 21:04:50,325] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 72% 18/25 [00:08<00:02,  3.02it/s][2024-03-29 21:04:50,329] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:51,067] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:51,181] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 80% 20/25 [00:09<00:01,  2.68it/s][2024-03-29 21:04:51,681] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 84% 21/25 [00:09<00:01,  2.48it/s][2024-03-29 21:04:51,769] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:51,780] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "[2024-03-29 21:04:52,028] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 92% 23/25 [00:09<00:00,  3.22it/s][2024-03-29 21:04:54,155] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 96% 24/25 [00:12<00:00,  1.40it/s][2024-03-29 21:04:54,467] [_client.py:1026] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "100% 25/25 [00:12<00:00,  2.02it/s]\n",
            "[2024-03-29 21:04:54,471] [record.py:360] Final report: {'counts/Correct': 21, 'counts/Incorrect': 4, 'score': 0.84}. Logged to /tmp/evallogs/240329210441MG44YXKV_gpt-3.5-turbo_spider-sql.jsonl\n",
            "[2024-03-29 21:04:54,471] [oaieval.py:229] Final report:\n",
            "[2024-03-29 21:04:54,471] [oaieval.py:231] counts/Correct: 21\n",
            "[2024-03-29 21:04:54,471] [oaieval.py:231] counts/Incorrect: 4\n",
            "[2024-03-29 21:04:54,471] [oaieval.py:231] score: 0.84\n",
            "[2024-03-29 21:04:54,488] [record.py:349] Logged 75 rows of events to /tmp/evallogs/240329210441MG44YXKV_gpt-3.5-turbo_spider-sql.jsonl: insert_time=15.229ms\n"
          ]
        }
      ],
      "source": [
        "!oaieval gpt-3.5-turbo spider-sql --max_samples 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dacEfWO6_j3q"
      },
      "source": [
        "`oaievalset` expects a model name and an eval set name, for which the valid options are specified in the YAML files under `evals/registry/eval_sets`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRctf_P_j3r"
      },
      "source": [
        "### Going through eval logs\n",
        "\n",
        "The eval logs are located at `/tmp/evallogs` and different log files are created for each evaluation run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Bv90jkjs_j3s",
        "outputId": "6e07ee9b-e950-43e6-ac14-8c023217275a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                spec  \\\n",
              "0  {'completion_fns': ['gpt-3.5-turbo'], 'eval_na...   \n",
              "1                                                NaN   \n",
              "2                                                NaN   \n",
              "3                                                NaN   \n",
              "4                                                NaN   \n",
              "\n",
              "                                        final_report                run_id  \\\n",
              "0                                                NaN                   NaN   \n",
              "1  {'counts/Correct': 21, 'counts/Incorrect': 4, ...                   NaN   \n",
              "2                                                NaN  240329210441MG44YXKV   \n",
              "3                                                NaN  240329210441MG44YXKV   \n",
              "4                                                NaN  240329210441MG44YXKV   \n",
              "\n",
              "   event_id           sample_id      type  \\\n",
              "0       NaN                 NaN       NaN   \n",
              "1       NaN                 NaN       NaN   \n",
              "2       0.0  spider-sql.dev.142  sampling   \n",
              "3       1.0   spider-sql.dev.72  sampling   \n",
              "4       2.0  spider-sql.dev.117  sampling   \n",
              "\n",
              "                                                data created_by  \\\n",
              "0                                                NaN        NaN   \n",
              "1                                                NaN        NaN   \n",
              "2  {'prompt': [{'content': 'Answer the following ...              \n",
              "3  {'prompt': [{'content': 'Answer the following ...              \n",
              "4  {'prompt': [{'content': 'Answer the following ...              \n",
              "\n",
              "                        created_at  \n",
              "0                              NaT  \n",
              "1                              NaT  \n",
              "2 2024-03-29 21:04:43.286984+00:00  \n",
              "3 2024-03-29 21:04:43.417651+00:00  \n",
              "4 2024-03-29 21:04:43.521920+00:00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-baa94d41-3668-43e7-bfe3-d95d6ece5a55\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spec</th>\n",
              "      <th>final_report</th>\n",
              "      <th>run_id</th>\n",
              "      <th>event_id</th>\n",
              "      <th>sample_id</th>\n",
              "      <th>type</th>\n",
              "      <th>data</th>\n",
              "      <th>created_by</th>\n",
              "      <th>created_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'completion_fns': ['gpt-3.5-turbo'], 'eval_na...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>{'counts/Correct': 21, 'counts/Incorrect': 4, ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>240329210441MG44YXKV</td>\n",
              "      <td>0.0</td>\n",
              "      <td>spider-sql.dev.142</td>\n",
              "      <td>sampling</td>\n",
              "      <td>{'prompt': [{'content': 'Answer the following ...</td>\n",
              "      <td></td>\n",
              "      <td>2024-03-29 21:04:43.286984+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>240329210441MG44YXKV</td>\n",
              "      <td>1.0</td>\n",
              "      <td>spider-sql.dev.72</td>\n",
              "      <td>sampling</td>\n",
              "      <td>{'prompt': [{'content': 'Answer the following ...</td>\n",
              "      <td></td>\n",
              "      <td>2024-03-29 21:04:43.417651+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>240329210441MG44YXKV</td>\n",
              "      <td>2.0</td>\n",
              "      <td>spider-sql.dev.117</td>\n",
              "      <td>sampling</td>\n",
              "      <td>{'prompt': [{'content': 'Answer the following ...</td>\n",
              "      <td></td>\n",
              "      <td>2024-03-29 21:04:43.521920+00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-baa94d41-3668-43e7-bfe3-d95d6ece5a55')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-baa94d41-3668-43e7-bfe3-d95d6ece5a55 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-baa94d41-3668-43e7-bfe3-d95d6ece5a55');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-193d8b42-82ae-4457-b2f2-b4e42573ba99\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-193d8b42-82ae-4457-b2f2-b4e42573ba99')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-193d8b42-82ae-4457-b2f2-b4e42573ba99 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"spec\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"final_report\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"run_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"240329210441MG44YXKV\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sample_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"spider-sql.dev.142\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"sampling\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_by\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"NaT\",\n        \"max\": \"NaT\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-03-29 21:04:43.286984+00:00\",\n        \"max\": \"2024-03-29 21:04:43.521920+00:00\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2024-03-29 21:04:43.286984+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "log_name = '240329210441MG44YXKV_gpt-3.5-turbo_spider-sql.jsonl' # \"EDIT THIS\" - copy from above\n",
        "events = f\"/tmp/evallogs/{log_name}\"\n",
        "display(pd.read_json(events, lines=True).head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EhEiWIkW_j3t"
      },
      "outputs": [],
      "source": [
        "# processing the log events generated by oaieval\n",
        "\n",
        "with open(events, \"r\") as f:\n",
        "    events_df = pd.read_json(f, lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZpvZJ4v_j3t"
      },
      "source": [
        "This file will contain structured logs of the evaluation. The first entry provides a detailed specification of the evaluation, including the completion functions, evaluation name, run configuration, creator’s name, run ID, and creation timestamp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R6Jxe-s-_j3u",
        "outputId": "05b37321-9b23-4ea4-a319-f9df4f291929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'completion_fns': ['gpt-3.5-turbo'],\n",
              " 'eval_name': 'spider-sql.dev.v0',\n",
              " 'base_eval': 'spider-sql',\n",
              " 'split': 'dev',\n",
              " 'run_config': {'completion_fns': ['gpt-3.5-turbo'],\n",
              "  'eval_spec': {'cls': 'evals.elsuite.modelgraded.classify:ModelBasedClassify',\n",
              "   'registry_path': '/usr/local/lib/python3.10/dist-packages/evals/registry',\n",
              "   'args': {'samples_jsonl': 'sql/spider_sql.jsonl',\n",
              "    'eval_type': 'cot_classify',\n",
              "    'modelgraded_spec': 'sql'},\n",
              "   'key': 'spider-sql.dev.v0',\n",
              "   'group': 'sql'},\n",
              "  'seed': 20220722,\n",
              "  'max_samples': 25,\n",
              "  'command': '/usr/local/bin/oaieval gpt-3.5-turbo spider-sql --max_samples 25',\n",
              "  'initial_settings': {'visible': False}},\n",
              " 'created_by': '',\n",
              " 'run_id': '240329210441MG44YXKV',\n",
              " 'created_at': '2024-03-29 21:04:41.851190'}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(events_df.iloc[0].spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFqGcqsw_j3v"
      },
      "source": [
        "Let's also look at the entry which provides the final report of the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mqCnqdv8_j3x",
        "outputId": "23a7f275-5ca2-44ca-a9f2-74da0d41ada7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'counts/Correct': 21, 'counts/Incorrect': 4, 'score': 0.84}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(events_df.dropna(subset=['final_report']).iloc[0]['final_report'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW4WC3MW_j30"
      },
      "source": [
        "We can also review individual evaluation events that provide specific samples (`sample_id`), results, event types, and metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "puDfWE4c_j31",
        "outputId": "1b902b6a-75d2-4fcb-c8b4-3df4ac809234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "run_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          240329210441MG44YXKV\n",
              "event_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.0\n",
              "sample_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         spider-sql.dev.142\n",
              "type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        sampling\n",
              "data          {'prompt': [{'content': 'Answer the following question with syntactically correct SQLite SQL. Be creative but the SQL must be correct.\n",
              "Use only the following tables and columns:\n",
              "Table: city. Columns: ID (number), Name (text), CountryCode (text), District (text), Population (number)\n",
              "Table: sqlite_sequence. Columns: name (text), seq (text)\n",
              "Table: country. Columns: Code (text), Name (text), Continent (text), Region (text), SurfaceArea (number), IndepYear (number), Population (number), LifeExpectancy (number), GNP (number), GNPOld (number), LocalName (text), GovernmentForm (text), HeadOfState (text), Capital (number), Code2 (text)\n",
              "Table: countrylanguage. Columns: CountryCode (text), Language (text), IsOfficial (text), Percentage (number)\n",
              "\n",
              "Question: How many countries have a republic as their form of government?\n",
              "', 'role': 'system'}], 'sampled': ['```sql\n",
              "SELECT COUNT(*) \n",
              "FROM country \n",
              "WHERE GovernmentForm = 'Republic';\n",
              "```']}\n",
              "created_at                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2024-03-29 21:04:43.286984+00:00\n",
              "Name: 2, dtype: object"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', None)  # None means no truncation\n",
        "display(events_df.iloc[2][['run_id', 'event_id', 'sample_id', 'type', 'data', 'created_at']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "msCIjjBF_j32",
        "outputId": "2e9f4b4e-8fd8-460a-9be9-b685eede4c9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: [{'content': 'Answer the following question with syntactically correct SQLite SQL. Be creative but the SQL must be correct.\\nUse only the following tables and columns:\\nTable: city. Columns: ID (number), Name (text), CountryCode (text), District (text), Population (number)\\nTable: sqlite_sequence. Columns: name (text), seq (text)\\nTable: country. Columns: Code (text), Name (text), Continent (text), Region (text), SurfaceArea (number), IndepYear (number), Population (number), LifeExpectancy (number), GNP (number), GNPOld (number), LocalName (text), GovernmentForm (text), HeadOfState (text), Capital (number), Code2 (text)\\nTable: countrylanguage. Columns: CountryCode (text), Language (text), IsOfficial (text), Percentage (number)\\n\\nQuestion: How many countries have a republic as their form of government?\\n', 'role': 'system'}]\n",
            "Sampled: [\"```sql\\nSELECT COUNT(*) \\nFROM country \\nWHERE GovernmentForm = 'Republic';\\n```\"]\n",
            "----------\n",
            "Prompt: [{'content': 'Answer the following question with syntactically correct SQLite SQL. Be creative but the SQL must be correct.\\nUse only the following tables and columns:\\nTable: museum. Columns: Museum_ID (number), Name (text), Num_of_Staff (number), Open_Year (text)\\nTable: visitor. Columns: ID (number), Name (text), Level_of_membership (number), Age (number)\\nTable: visit. Columns: Museum_ID (number), visitor_ID (text), Num_of_Ticket (number), Total_spent (number)\\n\\nQuestion: What is the average age of the visitors whose membership level is not higher than 4?\\n', 'role': 'system'}]\n",
            "Sampled: ['SELECT AVG(Age) \\nFROM visitor \\nWHERE Level_of_membership <= 4;']\n",
            "----------\n",
            "Prompt: [{'content': 'Answer the following question with syntactically correct SQLite SQL. Be creative but the SQL must be correct.\\nUse only the following tables and columns:\\nTable: TV_Channel. Columns: id (text), series_name (text), Country (text), Language (text), Content (text), Pixel_aspect_ratio_PAR (text), Hight_definition_TV (text), Pay_per_view_PPV (text), Package_Option (text)\\nTable: TV_series. Columns: id (number), Episode (text), Air_Date (text), Rating (text), Share (number), 18_49_Rating_Share (text), Viewers_m (text), Weekly_Rank (number), Channel (text)\\nTable: Cartoon. Columns: id (number), Title (text), Directed_by (text), Written_by (text), Original_air_date (text), Production_code (number), Channel (text)\\n\\nQuestion: What is the name and directors of all the cartoons that are ordered by air date?\\n', 'role': 'system'}]\n",
            "Sampled: ['SELECT Title, Directed_by\\nFROM Cartoon\\nORDER BY Original_air_date;']\n",
            "----------\n",
            "Prompt: [{'content': 'Answer the following question with syntactically correct SQLite SQL. Be creative but the SQL must be correct.\\nUse only the following tables and columns:\\nTable: battle. Columns: id (number), name (text), date (text), bulgarian_commander (text), latin_commander (text), result (text)\\nTable: ship. Columns: lost_in_battle (number), id (number), name (text), tonnage (text), ship_type (text), location (text), disposition_of_ship (text)\\nTable: death. Columns: caused_by_ship_id (number), id (number), note (text), killed (number), injured (number)\\n\\nQuestion: What is the average number of injuries caused each time?\\n', 'role': 'system'}]\n",
            "Sampled: ['SELECT AVG(injured) AS average_injuries_caused\\nFROM death;']\n",
            "----------\n",
            "Prompt: [{'content': 'Answer the following question with syntactically correct SQLite SQL. Be creative but the SQL must be correct.\\nUse only the following tables and columns:\\nTable: players. Columns: player_id (number), first_name (text), last_name (text), hand (text), birth_date (time), country_code (text)\\nTable: matches. Columns: best_of (number), draw_size (number), loser_age (number), loser_entry (text), loser_hand (text), loser_ht (number), loser_id (number), loser_ioc (text), loser_name (text), loser_rank (number), loser_rank_points (number), loser_seed (number), match_num (number), minutes (number), round (text), score (text), surface (text), tourney_date (time), tourney_id (text), tourney_level (text), tourney_name (text), winner_age (number), winner_entry (text), winner_hand (text), winner_ht (number), winner_id (number), winner_ioc (text), winner_name (text), winner_rank (number), winner_rank_points (number), winner_seed (number), year (number)\\nTable: rankings. Columns: ranking_date (time), ranking (number), player_id (number), ranking_points (number), tours (number)\\n\\nQuestion: Find the average rank of winners in all matches.\\n', 'role': 'system'}]\n",
            "Sampled: ['SELECT AVG(winner_rank) AS average_rank_of_winners\\nFROM matches;']\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "# Inspect samples\n",
        "for i, row in events_df[events_df['type'] == 'sampling'].head(5).iterrows():\n",
        "    data = pd.json_normalize(row['data'])\n",
        "    print(f\"Prompt: {data['prompt'].iloc[0]}\")\n",
        "    print(f\"Sampled: {data['sampled'].iloc[0]}\")\n",
        "    print(\"-\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6JSCEYr_j33"
      },
      "source": [
        "Let's review our failures to understand which tests did not succeed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7pCd7k10_j33"
      },
      "outputs": [],
      "source": [
        "def pretty_print_text(prompt):\n",
        "    # Define markers for the sections\n",
        "    markers = {\n",
        "        \"question\": \"[Question]:\",\n",
        "        \"expert\": \"[Expert]:\",\n",
        "        \"submission\": \"[Submission]:\",\n",
        "        \"end\": \"[END DATA]\"\n",
        "    }\n",
        "\n",
        "    # Function to extract text between markers\n",
        "    def extract_text(start_marker, end_marker):\n",
        "        start = prompt.find(start_marker) + len(start_marker)\n",
        "        end = prompt.find(end_marker)\n",
        "        text = prompt[start:end].strip()\n",
        "        if start_marker == markers[\"question\"]:\n",
        "            text = text.split(\"\\n\\nQuestion:\")[-1].strip() if \"\\n\\nQuestion:\" in text else text\n",
        "        elif start_marker == markers[\"submission\"]:\n",
        "            text = text.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
        "        return text\n",
        "\n",
        "    # Extracting text for each section\n",
        "    question_text = extract_text(markers[\"question\"], markers[\"expert\"])\n",
        "    expert_text = extract_text(markers[\"expert\"], markers[\"submission\"])\n",
        "    submission_text = extract_text(markers[\"submission\"], markers[\"end\"])\n",
        "\n",
        "    # HTML color codes and formatting\n",
        "    colors = {\n",
        "        \"question\": '<span style=\"color: #0000FF;\">QUESTION:<br>',\n",
        "        \"expert\": '<span style=\"color: #008000;\">EXPECTED:<br>',\n",
        "        \"submission\": '<span style=\"color: #FFA500;\">SUBMISSION:<br>'\n",
        "    }\n",
        "    color_end = '</span>'\n",
        "\n",
        "    # Display each section with color\n",
        "    from IPython.display import display, HTML\n",
        "    display(HTML(f\"{colors['question']}{question_text}{color_end}\"))\n",
        "    display(HTML(f\"{colors['expert']}{expert_text}{color_end}\"))\n",
        "    display(HTML(f\"{colors['submission']}{submission_text}{color_end}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sq5MPMPv_j34",
        "outputId": "a1967420-9682-4625-c9da-a5997c2a2191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #0000FF;\">QUESTION:<br>Which professionals live in the state of Indiana or have done treatment on more than 2 treatments? List his or her id, last name and cell phone.\n",
              "\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #008000;\">EXPECTED:<br>SELECT professional_id ,  last_name ,  cell_number FROM Professionals WHERE state  =  'Indiana' UNION SELECT T1.professional_id ,  T1.last_name ,  T1.cell_number FROM Professionals AS T1 JOIN Treatments AS T2 ON T1.professional_id  =  T2.professional_id GROUP BY T1.professional_id HAVING count(*)  >  2\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #FFA500;\">SUBMISSION:<br>SELECT professional_id, last_name, cell_number\n",
              "FROM Professionals\n",
              "WHERE state = 'Indiana'\n",
              "OR professional_id IN (\n",
              "    SELECT professional_id\n",
              "    FROM Treatments\n",
              "    GROUP BY professional_id\n",
              "    HAVING COUNT(*) > 2\n",
              ");\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #0000FF;\">QUESTION:<br>What is the continent name which Anguilla belongs to?\n",
              "\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #008000;\">EXPECTED:<br>SELECT Continent FROM country WHERE Name  =  \"Anguilla\"\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #FFA500;\">SUBMISSION:<br>SELECT c.Continent\n",
              "FROM country c\n",
              "WHERE c.Code = 'AIA';\n",
              "\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #0000FF;\">QUESTION:<br>How many airlines do we have?\n",
              "\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #008000;\">EXPECTED:<br>SELECT count(*) FROM AIRLINES\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #FFA500;\">SUBMISSION:<br>SELECT COUNT(DISTINCT Airline) AS TotalAirlines\n",
              "FROM airlines;\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #0000FF;\">QUESTION:<br>Find the weight of the youngest dog.\n",
              "\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #008000;\">EXPECTED:<br>SELECT weight FROM pets ORDER BY pet_age LIMIT 1\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color: #FFA500;\">SUBMISSION:<br>SELECT MIN(weight) \n",
              "FROM Pets \n",
              "WHERE PetType = 'dog' \n",
              "ORDER BY pet_age \n",
              "LIMIT 1;\n",
              "\n",
              "************</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Inspect metrics where choice is made and print only the prompt, result, and expected result if the choice is incorrect\n",
        "for i, row in events_df[events_df['type'] == 'metrics'].iterrows():\n",
        "    if row['data']['choice'] == 'Incorrect':\n",
        "        # Get the previous row's data, which contains the prompt and the expected result\n",
        "        prev_row = events_df.iloc[i-1]\n",
        "        prompt = prev_row['data']['prompt'][0]['content'] if 'prompt' in prev_row['data'] and len(prev_row['data']['prompt']) > 0 else \"Prompt not available\"\n",
        "        expected_result = prev_row['data'].get('ideal', 'Expected result not provided')\n",
        "\n",
        "        # Current row's data will be the actual result\n",
        "        result = row['data'].get('result', 'Actual result not provided')\n",
        "\n",
        "        pretty_print_text(prompt)\n",
        "        print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM4qQpXV_j35"
      },
      "source": [
        "Reviewing some of the failures we see the following:\n",
        "* The second incorrect answer had an unnecessary join with the 'Templates' table. Our eval was able to accurately identify this and flag this as incorrect.\n",
        "* Few other answers have minor syntax differences that caused the answers to get flagged.\n",
        "  * In situations like this, it would be worthwhile exploring whether we should continue iterating on the prompt to ensure certain stylistic choices, or if we should modify the evaluation suite to capture this variation.\n",
        "  * This type of failure hints at the potential need for model-graded evals as a way to ensure accuracy in grading the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cWvJ62_j36"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIaB1o-2_j36"
      },
      "source": [
        "Building out effective evals is a core part of the development cycle of LLM-based applications. The OpenAI Evals framework provides the core structure of building evals out of the box, and allows you to quickly spin up new tests for your various use cases. In this guide, we demonstrated step-by-step how to create an eval, run it, and analyze the results.\n",
        "\n",
        "The example shown in this guide represent a straightfoward use case for evals. As you continue to explore this framework, we recommend you explore creating more complex model-graded evals for actual production use cases. Happy evaluating!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}