{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Trees**\n",
        "A short notebook to show the high variance of decision trees while using cost complexity pruning. We then compare different impurity functions."
      ],
      "metadata": {
        "id": "37sApRAJS2z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Variance of decision trees**"
      ],
      "metadata": {
        "id": "umnXQ2__bNJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMlrJM2sPhnF"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Generate simulated data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "wTfKcFNvPiu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Train and prune decision tree using cross-validation\n",
        "def prune_tree_with_ccp_alpha(X, y):\n",
        "    dt = DecisionTreeClassifier(random_state=42)\n",
        "    path = dt.cost_complexity_pruning_path(X, y)\n",
        "    ccp_alphas = path.ccp_alphas\n",
        "    cv_scores = []\n",
        "\n",
        "    for ccp_alpha in ccp_alphas:\n",
        "        dt = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "        scores = cross_val_score(dt, X, y, cv=10, scoring='roc_auc')\n",
        "        cv_scores.append(scores.mean())\n",
        "\n",
        "    optimal_ccp_alpha = ccp_alphas[np.argmax(cv_scores)]\n",
        "    pruned_tree = DecisionTreeClassifier(random_state=42, ccp_alpha=optimal_ccp_alpha)\n",
        "    pruned_tree.fit(X, y)\n",
        "    return pruned_tree, optimal_ccp_alpha\n",
        "\n",
        "pruned_tree, optimal_ccp_alpha = prune_tree_with_ccp_alpha(X_train, y_train)\n",
        "print(f\"Optimal ccp_alpha: {optimal_ccp_alpha}\")"
      ],
      "metadata": {
        "id": "oiRmDafPPixZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Show that changing slightly training data, the final decision trees varies a lot\n",
        "\n",
        "# introduce a slight change in the training data\n",
        "X_train_modified = X_train + np.random.normal(0, 0.1, X_train.shape)\n",
        "\n",
        "# train the (pruned) tree on \"perturbed data\"\n",
        "pruned_tree_modified, optimal_ccp_alpha_modified = prune_tree_with_ccp_alpha(X_train_modified, y_train)\n",
        "\n",
        "# show the optimal ccp_alpha for the prunted tree on perturbed data\n",
        "print(f\"Optimal ccp_alpha on perturbed training data: {optimal_ccp_alpha_modified}\")"
      ],
      "metadata": {
        "id": "VwWa42mPPiz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of terminal nodes and depth\n",
        "print(f\"Original Decision Tree - Number of terminal nodes: {pruned_tree.get_n_leaves()}, Depth: {pruned_tree.get_depth()}\")\n",
        "print(f\"Modified Decision Tree - Number of terminal nodes: {pruned_tree_modified.get_n_leaves()}, Depth: {pruned_tree_modified.get_depth()}\")"
      ],
      "metadata": {
        "id": "5DfGjDCLSB-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot original and modified trees\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_tree(pruned_tree, filled=True)\n",
        "plt.title(\"Original Decision Tree\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_tree(pruned_tree_modified, filled=True)\n",
        "plt.title(\"Modified Decision Tree (Perturbed Data)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rCKpYnSpPi2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Compare results with respect to logistic regression\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# 5. Compute performance on test data using AUC\n",
        "dt_pred_proba = pruned_tree.predict_proba(X_test)[:, 1]\n",
        "dt_modified_pred_proba = pruned_tree_modified.predict_proba(X_test)[:, 1]\n",
        "log_reg_pred_proba = log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "dt_auc = roc_auc_score(y_test, dt_pred_proba)\n",
        "dt_modified_auc = roc_auc_score(y_test, dt_modified_pred_proba)\n",
        "log_reg_auc = roc_auc_score(y_test, log_reg_pred_proba)\n",
        "\n",
        "# compare test performance\n",
        "print(f\"Decision Tree AUC: {dt_auc}\")\n",
        "print(f\"Modified Decision Tree AUC: {dt_modified_auc}\")\n",
        "print(f\"Logistic Regression AUC: {log_reg_auc}\")\n",
        "print()\n",
        "\n",
        "# Compare ccp_alpha's\n",
        "print(f\"Original Decision Tree ccp_alpha: {optimal_ccp_alpha}\")\n",
        "print(f\"Modified Decision Tree ccp_alpha: {optimal_ccp_alpha_modified}\")\n",
        "print(f\"Difference in ccp_alpha: {abs(optimal_ccp_alpha - optimal_ccp_alpha_modified)}\")"
      ],
      "metadata": {
        "id": "eEwCkNrwPi46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Impurity functions**\n",
        "We check the impact of the impurity function on the structure and performance of the decision trees."
      ],
      "metadata": {
        "id": "Th4ux6V1bVWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prune decision tree using cross-validation with different impurity functions\n",
        "def prune_tree_with_ccp_alpha(X, y, criterion):\n",
        "    dt = DecisionTreeClassifier(criterion=criterion, random_state=42)\n",
        "    path = dt.cost_complexity_pruning_path(X, y)\n",
        "    ccp_alphas = path.ccp_alphas\n",
        "    cv_scores = []\n",
        "\n",
        "    for ccp_alpha in ccp_alphas:\n",
        "        dt = DecisionTreeClassifier(criterion=criterion, random_state=42, ccp_alpha=ccp_alpha)\n",
        "        scores = cross_val_score(dt, X, y, cv=10, scoring='roc_auc')\n",
        "        cv_scores.append(scores.mean())\n",
        "\n",
        "    optimal_ccp_alpha = ccp_alphas[np.argmax(cv_scores)]\n",
        "    pruned_tree = DecisionTreeClassifier(criterion=criterion, random_state=42, ccp_alpha=optimal_ccp_alpha)\n",
        "    pruned_tree.fit(X, y)\n",
        "    return pruned_tree, optimal_ccp_alpha"
      ],
      "metadata": {
        "id": "dH_hNjvDPi7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and prune trees with different impurity functions\n",
        "criteria = ['gini', 'entropy']\n",
        "pruned_trees = {}\n",
        "optimal_ccp_alphas = {}\n",
        "\n",
        "for criterion in criteria:\n",
        "    pruned_tree, optimal_ccp_alpha = prune_tree_with_ccp_alpha(X_train, y_train, criterion)\n",
        "    pruned_trees[criterion] = pruned_tree\n",
        "    optimal_ccp_alphas[criterion] = optimal_ccp_alpha\n",
        "    print(f\"Criterion: {criterion}, Optimal ccp_alpha: {optimal_ccp_alpha}\")"
      ],
      "metadata": {
        "id": "7fKOfnQXPi97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot trees for each criterion\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "for i, criterion in enumerate(criteria):\n",
        "    plt.subplot(1, len(criteria), i + 1)\n",
        "    plot_tree(pruned_trees[criterion], filled=True)\n",
        "    plt.title(f\"Decision Tree ({criterion})\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P-DAghQ4PjAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of terminal nodes and depth for each criterion\n",
        "for criterion in criteria:\n",
        "    tree = pruned_trees[criterion]\n",
        "    print(f\"\\nDecision Tree ({criterion})\")\n",
        "    print(f\"Number of terminal nodes: {tree.get_n_leaves()}\")\n",
        "    print(f\"Depth: {tree.get_depth()}\")"
      ],
      "metadata": {
        "id": "AZ2JOHIDPjDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare results with logistic regression\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Compute performance on test data using AUC\n",
        "auc_scores = {}\n",
        "log_reg_pred_proba = log_reg.predict_proba(X_test)[:, 1]\n",
        "log_reg_auc = roc_auc_score(y_test, log_reg_pred_proba)\n",
        "\n",
        "for criterion in criteria:\n",
        "    dt_pred_proba = pruned_trees[criterion].predict_proba(X_test)[:, 1]\n",
        "    auc_scores[criterion] = roc_auc_score(y_test, dt_pred_proba)\n",
        "\n",
        "# Print AUC scores\n",
        "print(\"\\nPerformance Comparison\")\n",
        "for criterion in criteria:\n",
        "    print(f\"Decision Tree ({criterion}) AUC: {auc_scores[criterion]}\")\n",
        "print(f\"Logistic Regression AUC: {log_reg_auc}\")\n",
        "\n",
        "# Compare trees\n",
        "print(\"\\nTree Comparison\")\n",
        "for criterion in criteria:\n",
        "    print(f\"Decision Tree ({criterion}) ccp_alpha: {optimal_ccp_alphas[criterion]}\")"
      ],
      "metadata": {
        "id": "U9vtbxXPPjF6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}