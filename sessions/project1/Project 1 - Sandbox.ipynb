{"cells":[{"cell_type":"markdown","metadata":{"id":"a7Bf9ZaIwstM"},"source":["### Introduction to Machine Learning in Finance and Insurance (Spring 2024)\n","# Project 1 - Credit Analytics - Sandbox"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6HIkJq78GSA"},"outputs":[],"source":["# Import basic libraries\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"lvgUrwxX8GSB"},"source":["# Implement a logistic regression model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"seZfZQ0n8GSC"},"outputs":[],"source":["# Import libraries\n","from sklearn.datasets import make_classification # toy dataset\n","from sklearn.linear_model import LogisticRegression # logistic regression model\n","from sklearn.metrics import log_loss # cross-entropy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"myAkFsvViXUx"},"outputs":[],"source":["# Fix random seed for reproducibility\n","np.random.seed(20)\n","\n","# Generate a toy dataset for binary classification (labels: 0 and 1)\n","n_samples = 50000\n","X, y = make_classification(n_samples=n_samples,\n","                           n_features=2,\n","                           n_informative=2,\n","                           n_redundant=0,\n","                           n_classes=2,\n","                           n_clusters_per_class=1,\n","                           weights=[0.95, 0.05])\n","\n","# Train-test split\n","m = int(0.8*n_samples) # number of samples in training set\n","n = n_samples - m # number of samples in test set\n","\n","X_train = X[:m, :]\n","y_train = y[:m]\n","\n","X_test = X[m:, :]\n","y_test = y[m:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljG96K898GSD"},"outputs":[],"source":["# Since this is a two-dimensional dataset, we can plot it!\n","# Yellow points have label 1 and blue points have label 0\n","plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker='.')\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhEGcacF8GSD"},"outputs":[],"source":["# Implement and train a logistic regression model\n","# Attetion! Set the argument panalty=None to perform a logistic regression *without* regularization\n","logistic_regression = LogisticRegression(penalty=None).fit(X, y)\n","\n","# Look at the parameters of the fitted model\n","print('Coefficients:', logistic_regression.coef_)\n","print('Intercept:', logistic_regression.intercept_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0XREFisA8GSE"},"outputs":[],"source":["# Predict probabilities of class one (i.e. yellow point) for each point in the test data set\n","prob_pred_one = logistic_regression.predict_proba(X_test)[:, 1]\n","\n","# Plot the histogram of the probabilities\n","plt.hist(prob_pred_one, bins=100)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7plh1W78GSE"},"outputs":[],"source":["# Predict label for each point in the test data set\n","# By deafult sklearn implements a threshold of 0.5\n","y_pred = logistic_regression.predict(X_test)\n","\n","plt.hist(y_pred)\n","plt.show()\n","\n","print(f'Frequency of yellow points: {100*np.mean(y_pred):.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fjWiwgd8GSF"},"outputs":[],"source":["# Compute the cross-entropy on the train and test data sets\n","\n","print(f'Cross-entropy (train): {log_loss(y_train, logistic_regression.predict_proba(X_train)[:, 1]):.4f}')\n","print(f'Cross-entropy (test): {log_loss(y_test, logistic_regression.predict_proba(X_test)[:, 1]):.4f}')"]},{"cell_type":"markdown","metadata":{"id":"D-KkMIdu8GSF"},"source":["# Implement a neural network using Keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ff0V_6uS8GSF"},"outputs":[],"source":["import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hXRdYzj8GSG"},"outputs":[],"source":["# Implement the network\n","# Two hidden layers with 20 neurons each with ReLU activativation function\n","# Output layer with sigmoid activation function (needed to output probabilities!)\n","neural_network = keras.Sequential([keras.layers.Dense(20, activation='relu'),\n","                                   keras.layers.Dense(20, activation='relu'),\n","                                   keras.layers.Dense(1, activation='sigmoid')])\n","\n","# Compile the network\n","# We choose Adam as gradient descent algorithm and cross-entropy as loss function\n","lr = 0.001 # learning rate\n","neural_network.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n","                       loss=keras.losses.BinaryCrossentropy())"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"QQsGiZQC8GSG"},"outputs":[],"source":["# Train the model\n","batch_size = 1024 # size of batches in stochastic gradient descent\n","epochs = 50 # number of iterations on dataset\n","history = neural_network.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gm8lzX138GSG"},"outputs":[],"source":["# Plot the cross-entropy loss during training\n","\n","plt.plot(history.history['loss'], 'b.-')\n","plt.ylabel('Cross-entropy loss (training data set)')\n","plt.xlabel('Number of epochs')\n","plt.yscale('log')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zjBvCNt8GSG"},"outputs":[],"source":["# Compute the cross-entropy on the train and test data sets\n","\n","print(f'Cross-entropy (train): {log_loss(y_train, neural_network.predict(X_train)):.4f}')\n","print(f'Cross-entropy (test): {log_loss(y_test, neural_network.predict(X_test)):.4f}')"]},{"cell_type":"markdown","metadata":{"id":"rln_eTne8GSG"},"source":["# Plotting ROC curves and computing AUC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sk4aUU888GSG"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, roc_curve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cE63lZJC8GSH"},"outputs":[],"source":["# Compute FPR and TPR for logistic regression\n","fpr_LR, tpr_LR, thresholds_LR = roc_curve(y_test, logistic_regression.predict_proba(X_test)[:, 1], pos_label=1)\n","auc_LR = roc_auc_score(y_test, logistic_regression.predict_proba(X_test)[:, 1])\n","\n","# Compute FPR and TPR for neural network\n","fpr_NN, tpr_NN, thresholds_NN = roc_curve(y_test, neural_network.predict(X_test), pos_label=1)\n","auc_NN = roc_auc_score(y_test, neural_network.predict(X_test))\n","\n","# Create plot\n","plt.figure(figsize= [8, 8])\n","plt.plot(fpr_LR, tpr_LR, 'r-', label='Logistic regression, AUC={:.3f}'.format(auc_LR))\n","plt.plot(fpr_NN, tpr_NN, 'b--', label='Neural network, AUC={:.3f}'.format(auc_NN))\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rQOoBpxa8GSH"},"source":["# Compute Value at Risk (VaR)"]},{"cell_type":"markdown","metadata":{"id":"CmqlTNge8GSH"},"source":["The Value at Risk (VaR) of a random variable $X$ at level $\\alpha$ is defined as:\n","\n","$$ \\text{VaR}(\\alpha) = \\inf \\{ t \\in \\mathbb{R} \\: : \\: \\mathbb{P}(X > t) \\le 1 - \\alpha \\} $$\n","\n","If $X$ denotes a distribution of financial losses (for example, minus P\\&L of an investment), then the probability that the losses exceed $\\text{VaR}(95\\%)$ is at most 5%:\n","\n","$$ \\text{VaR}(95\\%) = \\text{smallest value $t$ such that $\\mathbb{P}(\\text{Losses} > t) \\le 5\\%$} $$\n","\n","Equivalently: if we set aside an amount of money equal to $\\text{VaR}(95\\%)$, then we are sure that we can cover all losses at least 95\\% of the times."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JK9_MU508GSH"},"outputs":[],"source":["def var(x, alpha):\n","    n = len(x)\n","    # sort x in decreasing order\n","    sorted_x = np.sort(x)[::-1]\n","    return sorted_x[int(np.floor(n*(1-alpha)))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXFlx8oD8GSH"},"outputs":[],"source":["np.random.seed(100)\n","x = np.random.normal(0, 1, size=10000)\n","plt.hist(x, bins=200)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ENiq9ma8GSH"},"outputs":[],"source":["# Compute VaR(95%)\n","alpha = 0.95\n","print('VaR(95%):', var(x, alpha))\n","\n","# Plot VaR(95%) on top on histogram\n","plt.hist(x, bins=200)\n","plt.vlines(var(x, alpha), 0, 200, color='red', linestyles='--')\n","plt.ylim([0, 175])\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}