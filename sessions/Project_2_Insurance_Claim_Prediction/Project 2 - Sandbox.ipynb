{"cells":[{"cell_type":"markdown","metadata":{"id":"a7Bf9ZaIwstM"},"source":["### Introduction to Machine Learning in Finance and Insurance (Spring 2024)\n","# Project 2 - Insurance Claim Prediction - Sandbox"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"w6HIkJq78GSA","executionInfo":{"status":"ok","timestamp":1712928587906,"user_tz":-120,"elapsed":1,"user":{"displayName":"Andreas Schaufelbühl","userId":"02703351307249359741"}}},"outputs":[],"source":["# Import basic libraries\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"lvgUrwxX8GSB"},"source":["# Read a csv file using pandas"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"seZfZQ0n8GSC","executionInfo":{"status":"ok","timestamp":1712928591190,"user_tz":-120,"elapsed":776,"user":{"displayName":"Andreas Schaufelbühl","userId":"02703351307249359741"}}},"outputs":[],"source":["# Import libraries\n","# Pandas is a package used for data manipulation (e.g. dataframes, databases, etc)\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"myAkFsvViXUx","executionInfo":{"status":"error","timestamp":1712928591864,"user_tz":-120,"elapsed":343,"user":{"displayName":"Andreas Schaufelbühl","userId":"02703351307249359741"}},"outputId":"b0f19ce8-3129-4a67-ec47-2df992884740","colab":{"base_uri":"https://localhost:8080/","height":318}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'freMTPL2freq.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-85c2e13240bb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load dataset from csv file into pandas dataframe object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'freMTPL2freq.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'freMTPL2freq.csv'"]}],"source":["# Load dataset from csv file into pandas dataframe object\n","df = pd.read_csv('freMTPL2freq.csv', sep=';', decimal=',')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljG96K898GSD"},"outputs":[],"source":["# Inspect the first few rows of the dataframe\n","df"]},{"cell_type":"markdown","metadata":{"id":"LVTx22yRfHzg"},"source":["# Pre-process dataset features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGShvY15fHzg"},"outputs":[],"source":["# Define the pre-processing function for VehAge\n","# Attention! This is just an example. For your project submission, you must modify this function according to instructions.\n","\n","def pre_process_VehAge(x):\n","\n","    if x >= 0 and x < 6:\n","        output = 0\n","    else:\n","        output = 1\n","\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mc2svNCvfHzg"},"outputs":[],"source":["Exposure = df['Exposure']\n","\n","# Transform discrete/continuous variables\n","VehPower = np.log(df['VehPower'])\n","DrivAge = np.log(df['DrivAge'])\n","BonusMalus = np.log(df['BonusMalus'])\n","\n","# Apply pre-processing function to VehAge and one-hot encode it\n","VehAge = pd.get_dummies(df['VehAge'].apply(pre_process_VehAge))\n","\n","# Re-assemble the dataset by concatenating vertically the transformed features\n","X = np.float32(pd.concat([Exposure, VehPower, VehAge, DrivAge, BonusMalus], axis=1).values)\n","# Define the target labels (i.e. claim frequency)\n","y = np.float32(df['ClaimNb'].values/df['Exposure'].values)\n","\n","# Attention! Since this is an example, we are keeping only some features of the original dataset.\n","# For your final submission, modify the code accordingly.\n","# Attetion! For the moment, we also keep `Exposure` as the first feature in the dataset, because we want to be able\n","# to split it during the train-test split together with the rest of the dataset. We will then remove it from the dataset\n","# before training (see the function `get_train_test_split` below)"]},{"cell_type":"markdown","metadata":{"id":"my2iOmYBfHzh"},"source":["# Train a Poisson GLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwLeM_uHfHzh"},"outputs":[],"source":["# Import libraries\n","# Poisson Regressor model\n","from sklearn import linear_model\n","\n","# sklearn's functions for train-test split and data standardization\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# losses\n","from sklearn.metrics import mean_absolute_error, mean_poisson_deviance, mean_squared_error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PNWWqVvfHzi"},"outputs":[],"source":["# Custom train-test split function to split and return the weights (i.e. feature Exposure)\n","def get_train_test_split(X, y):\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=5)\n","\n","    # Keep exposures\n","\n","    w_train = X_train[:, 0]\n","    X_train = X_train[:, 1:]\n","\n","    w_test = X_test[:, 0]\n","    X_test = X_test[:, 1:]\n","\n","    return X_train, X_test, y_train, y_test, w_train, w_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEZicb4VfHzi"},"outputs":[],"source":["# Perform a train-test split\n","X_train, X_test, y_train, y_test, w_train, w_test = get_train_test_split(X, y)\n","\n","# Standardize features\n","scaler = StandardScaler().fit(X_train)\n","\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6qwc1fTfHzi"},"outputs":[],"source":["# Train Poisson GLM\n","\n","# Set alpha=0 to train without regularization\n","glm = linear_model.PoissonRegressor(alpha=0.)\n","# Use the argument `sample_weight` to minimize the *weighted* Poisson deviance with the appropriate weights (i.e. feature Exposure)\n","glm.fit(X_train, y_train, sample_weight=w_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vX70eDTgfHzi"},"outputs":[],"source":["# Custom mean Poisson deviance to handle cases of zero y_true or zero y_pred\n","def weighted_poisson_deviance(y_true, y_pred, sample_weight=None):\n","\n","    log_y_true = np.zeros(len(y_true))\n","    log_y_true[y_true > 0] = np.log(y_true[y_true > 0])\n","\n","    log_y_pred = np.zeros(len(y_pred))\n","    log_y_pred[y_pred > 0] = np.log(y_pred[y_pred > 0])\n","\n","    loss = (1/np.sum(sample_weight))*np.sum(sample_weight * 2 * (y_pred - y_true - y_true * log_y_pred + y_true * log_y_true))\n","\n","    return loss\n","\n","# Function to print metrics\n","def print_metrics(y_true, y_pred, losses, losses_names, sample_weight=None):\n","    for i_loss, loss in enumerate(losses):\n","        print(losses_names[i_loss] + ':', loss(y_true, y_pred, sample_weight=sample_weight))\n","\n","\n","losses = [mean_absolute_error, mean_squared_error, weighted_poisson_deviance]\n","\n","losses_names = ['MAE', 'MSE', 'Poisson-Dev']\n","\n","print('Training data set')\n","print_metrics(y_train, glm.predict(X_train), losses, losses_names, sample_weight=w_train)\n","print('Test data set')\n","print_metrics(y_test, glm.predict(X_test), losses, losses_names, sample_weight=w_test)"]},{"cell_type":"markdown","metadata":{"id":"Uyck3rUkfHzi"},"source":["# Train a Poisson feedforward neural network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghLDmHgcfHzi"},"outputs":[],"source":["import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWGiIoxMfHzj"},"outputs":[],"source":["model = keras.Sequential([keras.layers.Dense(10, activation='relu'),\n","                          keras.layers.Dense(10, activation='relu'),\n","                          keras.layers.Dense(1, activation='exponential')])\n","\n","lr = 0.01\n","\n","# Choose Poisson deviance as loss\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n","              loss=keras.losses.Poisson(),\n","              weighted_metrics=[])"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"EFehm3CKfHzj"},"outputs":[],"source":["# Minimize the *weighted* Poisson deviance by training with w_train (i.e. Exposure) as sample weights\n","# Attention! w_train must be a dataframe object from pandas due to a bug in Keras (https://github.com/keras-team/keras/issues/14877)\n","history = model.fit(x=X_train,\n","                    y=y_train,\n","                    sample_weight=pd.Series(w_train).to_frame(),\n","                    batch_size=10000,\n","                    epochs=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqd5pUeifHzj"},"outputs":[],"source":["plt.plot(history.history['loss'], 'b-')\n","plt.yscale('log')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TX85KzMHfHzj"},"outputs":[],"source":["#train_preds = model.predict(X_train).reshape(-1)\n","test_preds = model.predict(X_test).reshape(-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcQHh-ixfHzj"},"outputs":[],"source":["# print('Training data set')\n","# print_metrics(y_train, train_preds, losses, losses_names, sample_weight=w_train)\n","print('Test data set')\n","print_metrics(y_test, test_preds, losses, losses_names, sample_weight=w_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z509yg2JfHzj"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}